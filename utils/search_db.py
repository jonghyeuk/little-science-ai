import pandas as pd
import os
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from utils.explain_topic import explain_topic
import anthropic  # OpenAI ëŒ€ì‹  anthropic ì‚¬ìš©
import re

# ğŸ“ ë‚´ë¶€ DB ê²½ë¡œ
DB_PATH = os.path.join("data", "ISEF Final DB.xlsx")

# ğŸ”¤ ISEF DB ì—´ ë§¤í•‘
COLUMN_MAP = {
    'Project Title': 'ì œëª©',
    'Year': 'ì—°ë„',
    'Category': 'ë¶„ì•¼',
    'Fair Country': 'êµ­ê°€',
    'Fair State': 'ì§€ì—­',
    'Awards': 'ìˆ˜ìƒ'
}

# ì „ì—­ ë³€ìˆ˜ - ì‚¬ì „ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
_DB_INITIALIZED = False
_PROCESSED_DB = None
_VECTORIZER = None
_TFIDF_MATRIX = None

# ì´ˆê¸°í™” í•¨ìˆ˜ - ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
@st.cache_data(ttl=86400, show_spinner=False)  # ìºì‹œ 24ì‹œê°„ ìœ ì§€
def initialize_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    try:
        # Excel íŒŒì¼ ë¡œë“œ
        df = pd.read_excel(DB_PATH)
        _PROCESSED_DB = df
        
        # ë²¡í„°í™” ë¯¸ë¦¬ ìˆ˜í–‰
        title_field = 'Project Title'
        corpus = df[title_field].fillna("").astype(str).tolist()
        
        _VECTORIZER = TfidfVectorizer(
            analyzer='word', 
            ngram_range=(1, 2),
            lowercase=True
        )
        
        _TFIDF_MATRIX = _VECTORIZER.fit_transform(corpus)
        _DB_INITIALIZED = True
        
        print(f"âœ… ë‚´ë¶€ DB ì´ˆê¸°í™” ì™„ë£Œ: {len(df)} ê°œ ë…¼ë¬¸ ë¡œë“œë¨")
        return True
    except Exception as e:
        print(f"âŒ ë‚´ë¶€ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ - ê°œì„ ë¨
def extract_keywords(text, top_n=5):
    # í…ìŠ¤íŠ¸ ì •ì œ ê°•í™”
    text = re.sub(r'[^\w\s]', '', text.lower())
    
    # í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ì œê±° ê°•í™”
    stopwords = [
        # í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸
        'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì˜', 'ëŠ”', 'ì€', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì™€', 'ê³¼', 'í•˜ê³ ',
        'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ì•„', 'ì•¼', 'ì´ì•¼', 'ë¼', 'ì´ë¼', 'ì—ìš”', 'ì˜ˆìš”', 'ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤',
        'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
        # ê¸°íƒ€ ë¶ˆìš©ì–´
        'ê·¸', 'ì €', 'ê²ƒ', 'ë°', 'ë“±', 'ë“¤', 'ë•Œ', 'ê³³', 'ì¤‘', 'ê°„', 'ë‚´', 'ì™¸', 'ì „', 'í›„', 'ìƒ', 'í•˜', 'ì¢Œ', 'ìš°',
        # ì˜ì–´ ë¶ˆìš©ì–´
        'the', 'of', 'and', 'a', 'to', 'in', 'is', 'that', 'for', 'on', 'with', 'as', 'be', 'by', 'from', 'at', 'or'
    ]
    
    # ë‹¨ì–´ ë¶„ë¦¬ ë° í•„í„°ë§
    words = []
    for word in text.split():
        # ê¸¸ì´ 2 ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸, ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì œì™¸
        if len(word) >= 2 and word not in stopwords and not word.isdigit():
            # í•œêµ­ì–´ëŠ” ì–´ê°„ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
            if any('\uAC00' <= char <= '\uD7A3' for char in word):  # í•œê¸€ í¬í•¨
                # ì¡°ì‚¬ ì œê±° (ê°„ë‹¨í•œ ê·œì¹™)
                if word.endswith(('ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ë¡œ', 'ì˜', 'ëŠ”', 'ì€')):
                    word = word[:-1]
                elif word.endswith(('ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'í•œí…Œ', 'ì—ìš”', 'ì˜ˆìš”')):
                    word = word[:-2]
                elif word.endswith(('ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤')):
                    word = word[:-3]
            words.append(word)
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_counts = {}
    for word in words:
        word_counts[word] = word_counts.get(word, 0) + 1
    
    # ë¹ˆë„ìˆœ ì •ë ¬
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
    top_keywords = [word for word, _ in sorted_words[:top_n]]
    
    return top_keywords

# íš¨ìœ¨ì ì¸ Claude ë²ˆì—­ í•¨ìˆ˜ - í‚¤ì›Œë“œë§Œ ë²ˆì—­
@st.cache_data(show_spinner=False, ttl=3600)
def claude_translate_keywords(keywords, tgt_lang="en") -> list:
    if not keywords:
        return []
        
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        keyword_text = ", ".join(keywords)
        
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=100,  # í‚¤ì›Œë“œ ë²ˆì—­ì€ ì§§ìœ¼ë‹ˆ ì ì€ í† í°
            system=f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ {tgt_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”.",
            messages=[
                {"role": "user", "content": keyword_text}
            ]
        )
        
        translated = response.content[0].text.strip()
        return [k.strip() for k in translated.split(',')]
    except Exception as e:
        st.warning(f"í‚¤ì›Œë“œ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}")
        return keywords

# ì¼ê´„ ì²˜ë¦¬ í•¨ìˆ˜ - ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì œëª© í•œ ë²ˆì— ì²˜ë¦¬ (Claude ë²„ì „)
@st.cache_data(show_spinner=False, ttl=3600)
def batch_infer_content(titles, categories=None):
    """ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì œëª©ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ Claude API í˜¸ì¶œ ìµœì†Œí™”"""
    if not titles:
        return []
        
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        
        # ë²”ì£¼ ì •ë³´ ì¶”ê°€
        prompts = []
        for i, title in enumerate(titles):
            category = categories[i] if categories and i < len(categories) else None
            if category:
                prompts.append(f"{i+1}. '{title}' (ì—°êµ¬ ë¶„ì•¼: {category})")
            else:
                prompts.append(f"{i+1}. '{title}'")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
        system_prompt = """
        ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë§Œìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ê·œì¹™:
        
        1. ì œëª©ë§Œ ìˆê³  ì‹¤ì œ ë…¼ë¬¸ì„ ë³´ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ëª…í™•íˆ ì–¸ê¸‰í•  ê²ƒ
        2. ì œëª©ì— ì—†ëŠ” êµ¬ì²´ì  ë°©ë²•ë¡ ì´ë‚˜ ê²°ê³¼ëŠ” ì¶”ë¡ í•˜ì§€ ë§ ê²ƒ  
        3. ì„œìˆ ì€ ë¨¼ì € ì œëª©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•˜ê³ , ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•œë‹¤. ì„¤ëª…í•˜ëŠ” ëª¨ë“  ì„œìˆ ì€ "~ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤" ê°™ì€ ì¶”ì¸¡í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        4. ê° ë…¼ë¬¸ì— ëŒ€í•´ 4-6ë¬¸ì¥ì˜ ì•Œê¸°ì‰¬ìš´ êµ¬ì²´ì ì¸ ì„¤ëª…ì„ ì œê³µí•˜ê³  ì˜ˆë¥¼ ë“œëŠ” ì„¤ëª…ì„ ë³´ê°•í•  í•  ê²ƒ
        5. ê° ì‘ë‹µ ì‹œì‘ì— ë²ˆí˜¸ë¥¼ ìœ ì§€í•  ê²ƒ (1., 2., ë“±)
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê° ë…¼ë¬¸ì„ ì„¤ëª…í•˜ì„¸ìš”:
        "X. ì´ ì—°êµ¬ëŠ” [ì£¼ì œ]ì— ê´€í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì œëª©ì—ì„œ ìœ ì¶”í•˜ë©´, [ì˜ˆìƒ ëª©ì /ë°©ë²•]ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. (ì£¼ì˜: ì´ëŠ” ì œëª©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ë¡ ìœ¼ë¡œ, ì‹¤ì œ ì—°êµ¬ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
        ë‚˜ìœ ì˜ˆ: "ì „ìê¸° ìœ ë„ì˜ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì „ê¸° ìƒì‚° ë°©ë²•ì„..."
        ì¢‹ì€ ì˜ˆ: "ìì˜ ìƒì²´ëª¨ë°©í•™ê³¼ ì „ìê¸° ìœ ë„ë¥¼ í™œìš©í•œ ì „ê¸° ìƒì‚° ì—°êµ¬ì…ë‹ˆë‹¤."
        """
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì‘ì„±
        user_prompt = "ë‹¤ìŒ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë“¤ì˜ ë‚´ìš©ì„ ì¶”ë¡ í•´ì£¼ì„¸ìš”:\n\n" + "\n".join(prompts)
        
        # Claude API í˜¸ì¶œ
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,  # ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì¼ê´„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ í† í°
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        full_response = response.content[0].text.strip()
        
        # ì‘ë‹µ íŒŒì‹±
        summaries = []
        current_summary = ""
        current_index = 0
        
        lines = full_response.split('\n')
        for line in lines:
            # ìƒˆë¡œìš´ í•­ëª© ì‹œì‘ ê°ì§€ (1., 2., ë“±)
            match = re.match(r'^(\d+)\.', line)
            if match:
                # ì´ì „ ìš”ì•½ ì €ì¥
                if current_summary and current_index > 0:
                    summaries.append(current_summary.strip())
                    
                # ìƒˆ ìš”ì•½ ì‹œì‘
                index = int(match.group(1))
                current_index = index
                current_summary = line
            elif current_summary:  # í˜„ì¬ ìš”ì•½ì— ë¼ì¸ ì¶”ê°€
                current_summary += " " + line
        
        # ë§ˆì§€ë§‰ ìš”ì•½ ì €ì¥
        if current_summary:
            summaries.append(current_summary.strip())
        
        # ìš”ì•½ ê°œìˆ˜ í™•ì¸ ë° ì¡°ì • 
        if len(summaries) < len(titles):
            # ë¶€ì¡±í•œ ìš”ì•½ ì¶”ê°€
            for i in range(len(summaries), len(titles)):
                title = titles[i]
                summaries.append(f"{i+1}. ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ìš”ì•½ ìƒì„± ì‹¤íŒ¨)")
        
        return summaries
    
    except Exception as e:
        print(f"ì¼ê´„ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ìš”ì•½ ë°˜í™˜
        return [f"ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ì¶”ë¡  ì˜¤ë¥˜)" for title in titles]

# ê°œë³„ í”„ë¡œì íŠ¸ ë‚´ìš© ì¶”ë¡  í•¨ìˆ˜ (Claude ë²„ì „)
@st.cache_data(show_spinner=False, ttl=3600)
def infer_project_content(title, category=None):
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        
        system_prompt = """
        ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë§Œìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ê·œì¹™:
        
        1. ì œëª©ë§Œ ìˆê³  ì‹¤ì œ ë…¼ë¬¸ì„ ë³´ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ëª…í™•íˆ ì–¸ê¸‰í•  ê²ƒ
        2. ëª¨ë“  ì„œìˆ ì€ "~ë¡œ ì¶”ì¸¡ë©ë‹ˆë‹¤", "~ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤" ê°™ì€ ì¶”ì¸¡í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        3. êµ¬ì²´ì ì¸ ë°©ë²•ë¡ ê³¼ ê²°ê³¼ëŠ” ì™„ì „íˆ ì¶”ì¸¡ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹œí•  ê²ƒ
        4. ì œëª©ì—ì„œ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ì—°êµ¬ ì£¼ì œì™€ ì˜ˆìƒë˜ëŠ” ì ‘ê·¼ë²• ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•  ê²ƒ
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
        "ì´ ì—°êµ¬ëŠ” [ì£¼ì œ]ì— ê´€í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì œëª©ì—ì„œ ìœ ì¶”í•˜ë©´, [ì˜ˆìƒ ëª©ì ]ì„ ìœ„í•´ [ì˜ˆìƒ ë°©ë²•]ì„ ì‚¬ìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” [ì˜ˆìƒ ë¶„ì•¼]ì— ê¸°ì—¬í•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, [ì ì¬ì  ì˜ì˜]ë¥¼ ê°€ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. (ì£¼ì˜: ì´ëŠ” ì œëª©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ë¡ ìœ¼ë¡œ, ì‹¤ì œ ì—°êµ¬ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
        """
        
        user_prompt = title
        if category:
            user_prompt = f"{title} (ì—°êµ¬ ë¶„ì•¼: {category})"
            
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=300,  # ê°œë³„ í”„ë¡œì íŠ¸ ì¶”ë¡ ì„ ìœ„í•œ ì ì ˆí•œ í† í°
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        return response.content[0].text.strip()
    except Exception as e:
        return f"ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ì¶”ë¡  ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.)"

# ë‚´ë¶€ DB ë¡œë“œ ë° ì •ì œ (ê¸°ì¡´ í•¨ìˆ˜ ë³´ì¡´)
@st.cache_data(ttl=3600)
def load_internal_db():
    try:
        df = pd.read_excel(DB_PATH)
        return df
    except Exception as e:
        st.error(f"âŒ ë‚´ë¶€ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ì£¼ì œ ê´€ë ¨ì„± ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
def is_topic_relevant(title, category, search_keywords, min_match_score=0.3):
    """ê²€ìƒ‰ ì£¼ì œì™€ ë…¼ë¬¸ì˜ ê´€ë ¨ì„±ì„ ê²€ì¦"""
    title_lower = title.lower()
    category_lower = category.lower() if category else ""
    
    # ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    match_score = 0
    total_keywords = len(search_keywords)
    
    for keyword in search_keywords:
        keyword_lower = keyword.lower()
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë¶€ë¶„ ë§¤ì¹­
        if keyword_lower in title_lower:
            match_score += 1.0  # ì™„ì „ ë§¤ì¹­
        elif any(k in keyword_lower for k in title_lower.split() if len(k) > 2):
            match_score += 0.5  # ë¶€ë¶„ ë§¤ì¹­
        
        # ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        if keyword_lower in category_lower:
            match_score += 0.3
    
    relevance_score = match_score / total_keywords if total_keywords > 0 else 0
    
    print(f"   ê´€ë ¨ì„± ê²€ì¦: '{title[:40]}...' = {relevance_score:.3f}")
    return relevance_score >= min_match_score

# í‚¤ì›Œë“œ í™•ì¥ í•¨ìˆ˜ ì¶”ê°€
def expand_search_keywords(keywords):
    """ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ìš©ì–´ ì¶”ê°€"""
    keyword_expansions = {
        'microplastic': ['microplastic', 'plastic', 'polymer', 'pollution', 'marine', 'ocean'],
        'plastic': ['plastic', 'polymer', 'microplastic', 'pollution', 'waste'],
        'ë¯¸ì„¸í”Œë¼ìŠ¤í‹±': ['plastic', 'microplastic', 'polymer', 'pollution'],
        'environment': ['environmental', 'ecology', 'pollution', 'marine', 'ocean'],
        'í™˜ê²½': ['environmental', 'ecology', 'pollution'],
        'pollution': ['pollution', 'contamination', 'waste', 'environmental'],
        'ì˜¤ì—¼': ['pollution', 'contamination', 'environmental'],
        'marine': ['marine', 'ocean', 'sea', 'water', 'aquatic'],
        'í•´ì–‘': ['marine', 'ocean', 'sea', 'water'],
        'health': ['health', 'medical', 'human', 'body', 'toxicity'],
        'ê±´ê°•': ['health', 'medical', 'human'],
        'water': ['water', 'aquatic', 'marine', 'ocean', 'sea'],
        'ë¬¼': ['water', 'aquatic', 'marine']
    }
    
    expanded = set(keywords)  # ì›ë³¸ í‚¤ì›Œë“œ ìœ ì§€
    
    for keyword in keywords:
        if keyword.lower() in keyword_expansions:
            expanded.update(keyword_expansions[keyword.lower()])
    
    return list(expanded)

# ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ - ëˆ„ë½ë˜ì—ˆë˜ í•¨ìˆ˜ ì¶”ê°€
def search_similar_titles(user_input: str, max_results: int = 10):
    """ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ - ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ìœ ì‚¬í•œ ë…¼ë¬¸ ì œëª©ë“¤ì„ ê²€ìƒ‰"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{user_input}'")
    
    # DB ì´ˆê¸°í™” í™•ì¸ ë° ìˆ˜í–‰
    if not _DB_INITIALIZED:
        initialize_db()
    
    # ê¸°ë³¸ DB ì‚¬ìš© (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)
    if _PROCESSED_DB is None:
        df = load_internal_db()
    else:
        df = _PROCESSED_DB
    
    if df.empty:
        print("   âŒ DBê°€ ë¹„ì–´ìˆìŒ")
        return []
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ - ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    print("ğŸ“ 1ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ")
    keywords = extract_keywords(user_input)
    print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
    
    if not keywords:
        print("   âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        return []
    
    # 2. í‚¤ì›Œë“œ ë²ˆì—­ (Claude API í˜¸ì¶œ 1íšŒ) - ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    print("ğŸŒ 2ë‹¨ê³„: í‚¤ì›Œë“œ ë²ˆì—­")
    translated_keywords = claude_translate_keywords(keywords)
    print(f"   ë²ˆì—­ëœ í‚¤ì›Œë“œ: {translated_keywords}")
    
    if not translated_keywords:
        translated_keywords = keywords
        print("   âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
    
    search_query = " ".join(translated_keywords)
    print(f"   ìµœì¢… ê²€ìƒ‰ì–´: '{search_query}'")
    
    # 3. ì˜ì–´ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰ (ì´ˆê¸°í™”ëœ ë²¡í„°ë¼ì´ì € ì‚¬ìš©) - ì˜¤ë¥˜ ìˆ˜ì •
    print("ğŸ”¢ 3ë‹¨ê³„: ìœ ì‚¬ë„ ê³„ì‚°")
    try:
        if _VECTORIZER is not None and _TFIDF_MATRIX is not None:  # ìˆ˜ì •: is not None ëª…ì‹œì  ë¹„êµ
            # ì‚¬ì „ ì²˜ë¦¬ëœ ë²¡í„°ë¼ì´ì €ì™€ ë§¤íŠ¸ë¦­ìŠ¤ ì‚¬ìš©
            search_vector = _VECTORIZER.transform([search_query])
            cosine_sim = cosine_similarity(search_vector, _TFIDF_MATRIX)[0]
            print(f"   ë²¡í„°í™” ì„±ê³µ, ì´ {len(cosine_sim)}ê°œ ë¬¸ì„œì™€ ë¹„êµ")
        else:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            print("   âš ï¸ ì‚¬ì „ ì²˜ë¦¬ëœ ë²¡í„° ì—†ìŒ, ìƒˆë¡œ ê³„ì‚°")
            title_field = 'Project Title'
            corpus = df[title_field].fillna("").astype(str).tolist()
            corpus.append(search_query)
            
            vectorizer = TfidfVectorizer(
                analyzer='word', 
                ngram_range=(1, 2),
                lowercase=True
            )
            
            tfidf_matrix = vectorizer.fit_transform(corpus)
            cosine_sim = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])[0]
            print(f"   ìƒˆë¡œ ë²¡í„°í™” ì™„ë£Œ, ì´ {len(cosine_sim)}ê°œ ë¬¸ì„œì™€ ë¹„êµ")
    except Exception as e:
        print(f"   âŒ ê²€ìƒ‰ ë²¡í„°í™” ì˜¤ë¥˜: {e}")
        return []
    
    # 4. ê²°ê³¼ ì •ë ¬ - ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    print("ğŸ“Š 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„")
    result_df = df.copy()
    result_df['score'] = cosine_sim
    
    # ìƒìœ„ 10ê°œ ì ìˆ˜ì™€ ì œëª© ì¶œë ¥
    top_10_scores = result_df.nlargest(10, 'score')[['Project Title', 'Category', 'score']]
    print("   ìƒìœ„ 10ê°œ ìœ ì‚¬ë„ ì ìˆ˜:")
    for idx, row in top_10_scores.iterrows():
        print(f"     {row['score']:.6f}: [{row.get('Category', 'N/A')}] {row['Project Title'][:60]}...")
    
    # ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
    thresholds = [0.1, 0.05, 0.01, 0.005]
    selected_threshold = 0.005  # ê¸°ë³¸ê°’
    for threshold in thresholds:
        filtered_count = len(result_df[result_df['score'] > threshold])
        print(f"   ì„ê³„ê°’ {threshold} ì´ìƒ: {filtered_count}ê°œ")
        if filtered_count > 0 and filtered_count <= max_results * 3:  # ì ë‹¹í•œ ìˆ˜ì˜ ê²°ê³¼
            selected_threshold = threshold
            break
    
    # ì„ íƒëœ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§
    filtered_df = result_df[result_df['score'] > selected_threshold].copy()
    print(f"   ì„ íƒëœ ì„ê³„ê°’: {selected_threshold}")
    
    if filtered_df.empty:
        print("   âŒ ê´€ë ¨ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return []
    
    # 5. ìƒìœ„ ê²°ê³¼ ì„ íƒ
    top_df = filtered_df.sort_values(by='score', ascending=False).head(max_results)
    print(f"   ìµœì¢… ì„ íƒ: {len(top_df)}ê°œ")
    
    print("ğŸ“‹ ì„ íƒëœ ë…¼ë¬¸ë“¤:")
    for idx, row in top_df.iterrows():
        print(f"     {row['score']:.6f}: [{row.get('Category', 'N/A')}] {row['Project Title']}")
    
    # 6. ê²°ê³¼ë¥¼ ìœ„í•œ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
    titles = []
    categories = []
    for _, row in top_df.iterrows():
        titles.append(row.get('Project Title', ''))
        categories.append(row.get('Category', ''))
    
    # 7. ì¼ê´„ ì²˜ë¦¬ë¡œ ëª¨ë“  ìš”ì•½ í•œ ë²ˆì— ìƒì„±
    if titles:
        print("ğŸ¤– 5ë‹¨ê³„: AI ìš”ì•½ ìƒì„±")
        summaries = batch_infer_content(titles, categories)
        print(f"   ìƒì„±ëœ ìš”ì•½: {len(summaries)}ê°œ")
    else:
        summaries = []
    
    # 8. ê²°ê³¼ êµ¬ì„±
    results = []
    for i, (_, row) in enumerate(top_df.iterrows()):
        project_title = row.get('Project Title', '')
        category = row.get('Category', '')
        
        # ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        summary = ""
        if i < len(summaries):
            # ìš”ì•½ì—ì„œ ë²ˆí˜¸ ì œê±°
            summary_text = summaries[i]
            summary = re.sub(r'^\d+\.\s*', '', summary_text)
        else:
            summary = f"ì´ í”„ë¡œì íŠ¸ëŠ” '{project_title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
        
        result_item = {
            'ì œëª©': project_title,  # ì˜ì–´ ì œëª© ì‚¬ìš©
            'ì—°ë„': str(row.get('Year', '')),
            'ë¶„ì•¼': category,
            'êµ­ê°€': row.get('Fair Country', ''),
            'ì§€ì—­': row.get('Fair State', ''),
            'ìˆ˜ìƒ': row.get('Awards', ''),
            'ìš”ì•½': summary,
            'score': float(row.get('score', 0))
        }
        
        results.append(result_item)
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
    return results
