import pandas as pd
import os
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import anthropic
import re

# ğŸ“ ë‚´ë¶€ DB ê²½ë¡œ
DB_PATH = os.path.join("data", "ISEF Final DB.xlsx")

# DB íŒŒì¼ ì¡´ì¬ í™•ì¸
print(f"ğŸ“ DB íŒŒì¼ í™•ì¸: {os.path.exists(DB_PATH)} ({DB_PATH})")

# ì „ì—­ ë³€ìˆ˜ - ì‚¬ì „ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
_DB_INITIALIZED = False
_PROCESSED_DB = None
_VECTORIZER = None
_TFIDF_MATRIX = None

# ì´ˆê¸°í™” í•¨ìˆ˜ - ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
@st.cache_data(ttl=86400, show_spinner=False)
def initialize_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    try:
        df = pd.read_excel(DB_PATH)
        _PROCESSED_DB = df
        
        # ë²¡í„°í™” ë¯¸ë¦¬ ìˆ˜í–‰
        corpus = df['Project Title'].fillna("").astype(str).tolist()
        
        _VECTORIZER = TfidfVectorizer(
            analyzer='word', 
            ngram_range=(1, 2),
            lowercase=True,
            max_features=5000  # ì„±ëŠ¥ ìµœì í™”
        )
        
        _TFIDF_MATRIX = _VECTORIZER.fit_transform(corpus)
        _DB_INITIALIZED = True
        
        print(f"âœ… ë‚´ë¶€ DB ì´ˆê¸°í™” ì™„ë£Œ: {len(df)} ê°œ ë…¼ë¬¸ ë¡œë“œë¨")
        return True
    except Exception as e:
        print(f"âŒ ë‚´ë¶€ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# ğŸ”¥ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ â†’ ì˜ì–´) - í™•ì¥ëœ ë²„ì „
def extract_and_translate_keywords(text):
    """í•œêµ­ì–´ ì…ë ¥ì„ ì˜ì–´ í‚¤ì›Œë“œë¡œ ë³€í™˜"""
    # ğŸ”¥ í™•ì¥ëœ ë§¤í•‘ í…Œì´ë¸”
    keyword_map = {
        # ìš´ë™/ê±´ê°• ê´€ë ¨ - í™•ì¥
        'ìš´ë™': 'exercise physical activity fitness training workout',
        'ì²´ì§€ë°©': 'body fat weight loss adipose tissue',
        'ê°ëŸ‰': 'weight loss reduction decrease',
        'ë‹¤ì´ì–´íŠ¸': 'diet weight loss nutrition dietary',
        'ê·¼ìœ¡': 'muscle strength training resistance',
        'ê±´ê°•': 'health wellness medical fitness',
        'ìŠ¤í¬ì¸ ': 'sports athletics performance competition',
        'ë¹„ë§Œ': 'obesity overweight BMI body mass',
        'ì‹ì´': 'dietary nutrition food eating',
        'ì¹¼ë¡œë¦¬': 'calorie energy metabolism burn',
        'ê·¼ë ¥': 'strength resistance training power',
        'ì§€êµ¬ë ¥': 'endurance cardio aerobic stamina',
        'í—¬ìŠ¤': 'fitness health wellness gym',
        'íŠ¸ë ˆì´ë‹': 'training exercise workout routine',
        'ì²´ì¤‘': 'weight body mass scale',
        'ì‹ ì§„ëŒ€ì‚¬': 'metabolism metabolic rate energy',
        
        # í™˜ê²½ ê´€ë ¨
        'í™˜ê²½': 'environment environmental pollution ecology',
        'ì˜¤ì—¼': 'pollution contamination environmental waste',
        'ë¯¸ì„¸í”Œë¼ìŠ¤í‹±': 'microplastic plastic pollution marine ocean',
        'ê¸°í›„': 'climate change global warming temperature',
        'ì¬í™œìš©': 'recycling waste management sustainability',
        'ì§€êµ¬ì˜¨ë‚œí™”': 'global warming climate change temperature',
        'ìƒíƒœê³„': 'ecosystem ecological environment biodiversity',
        
        # ì—ë„ˆì§€ ê´€ë ¨
        'íƒœì–‘ê´‘': 'solar energy renewable photovoltaic panel',
        'ì‹ ì¬ìƒ': 'renewable energy sustainable green',
        'ë°°í„°ë¦¬': 'battery energy storage power cell',
        'ì—°ë£Œì „ì§€': 'fuel cell hydrogen energy power',
        'ì „ê¸°': 'electricity electrical power energy',
        'ë°œì „': 'power generation electricity energy',
        
        # ìƒë¬¼í•™ ê´€ë ¨
        'ìœ ì „ì': 'gene genetic DNA molecular biology',
        'ì„¸í¬': 'cell cellular biology molecular membrane',
        'í•­ìƒì œ': 'antibiotic antimicrobial resistance bacteria',
        'ë°”ì´ëŸ¬ìŠ¤': 'virus viral infection disease pathogen',
        'ë°•í…Œë¦¬ì•„': 'bacteria bacterial microbiology pathogen',
        'ë‹¨ë°±ì§ˆ': 'protein molecular biology biochemistry',
        'íš¨ì†Œ': 'enzyme biochemistry catalysis reaction',
        
        # í™”í•™ ê´€ë ¨
        'í™”í•™': 'chemistry chemical reaction synthesis compound',
        'ì´‰ë§¤': 'catalyst catalysis chemical reaction',
        'ë‚˜ë…¸': 'nano nanotechnology materials science',
        'ë¶„ì': 'molecule molecular chemistry structure',
        'ë°˜ì‘': 'reaction chemical synthesis process',
        
        # ë¬¼ë¦¬í•™ ê´€ë ¨
        'ë¬¼ë¦¬': 'physics mechanical quantum electromagnetic',
        'ì „ì': 'electronics electronic circuit sensor device',
        'ë¡œë´‡': 'robot robotics automation artificial intelligence',
        'ì„¼ì„œ': 'sensor detection measurement device monitoring',
        'ê´‘í•™': 'optics optical light laser photon',
        
        # ì»´í“¨í„°/AI ê´€ë ¨
        'ì¸ê³µì§€ëŠ¥': 'artificial intelligence machine learning AI neural',
        'ë”¥ëŸ¬ë‹': 'deep learning neural network AI',
        'ì•±': 'application software mobile technology',
        'ë°ì´í„°': 'data analysis statistics information',
        'ì•Œê³ ë¦¬ì¦˜': 'algorithm computational programming',
        
        # ì˜í•™ ê´€ë ¨
        'ì˜í•™': 'medicine medical health clinical',
        'ì¹˜ë£Œ': 'treatment therapy medical healing',
        'ì•½ë¬¼': 'drug pharmaceutical medicine therapy',
        'ì§ˆë³‘': 'disease illness medical pathology',
        'ì§„ë‹¨': 'diagnosis medical detection screening'
    }
    
    # ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
    text_lower = text.lower()
    matched_keywords = []
    
    print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸ ë¶„ì„: '{text_lower}'")
    
    for korean, english in keyword_map.items():
        if korean in text_lower:
            matched_keywords.extend(english.split())
            print(f"   ë§¤ì¹­: '{korean}' â†’ {english.split()}")
    
    # ê¸°ë³¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬
    if not matched_keywords:
        # ì˜ì–´ ë‹¨ì–´ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
        english_words = re.findall(r'[a-zA-Z]+', text)
        matched_keywords.extend(english_words)
        print(f"   ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ: {english_words}")
        
        # í•œêµ­ì–´ ë‹¨ì–´ë„ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ì¼ë¶€ ë…¼ë¬¸ ì œëª©ì´ í•œêµ­ì–´ì¼ ìˆ˜ ìˆìŒ)
        korean_words = re.findall(r'[ê°€-í£]+', text)
        if korean_words:
            matched_keywords.extend(korean_words)
            print(f"   í•œêµ­ì–´ ë‹¨ì–´ ì¶”ê°€: {korean_words}")
        
        # ğŸ”¥ ì¶”ê°€: ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ëª¨ë“  ë‹¨ì–´ í¬í•¨
        all_words = text.replace(',', ' ').replace('.', ' ').split()
        for word in all_words:
            if len(word) >= 2:
                matched_keywords.append(word)
        print(f"   ëª¨ë“  ë‹¨ì–´ ì¶”ê°€: {all_words}")
    
    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œë¡œ í™•ì¥ (ë” ë§ì€ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€)
    unique_keywords = list(set(matched_keywords))[:10]
    
    print(f"ğŸ” í‚¤ì›Œë“œ ë³€í™˜: '{text}' â†’ {unique_keywords}")
    return unique_keywords

# ğŸ¤– ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
@st.cache_data(show_spinner=False, ttl=3600)
def generate_simple_summary(title, category=None, index=1):
    """ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±"""
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        
        prompt = f"ì œëª©: '{title}'"
        if category:
            prompt += f" (ë¶„ì•¼: {category})"
        prompt += "\n\nìœ„ ê³¼í•™ í”„ë¡œì íŠ¸ ì œëª©ì„ ë³´ê³  3-4ë¬¸ì¥ìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”. '~ë¡œ ì¶”ì •ë©ë‹ˆë‹¤' í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=200,
            temperature=0.3,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.content[0].text.strip()
        
    except Exception as e:
        print(f"âš ï¸ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."

# ğŸ¯ ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ - ë””ë²„ê¹… ê°•í™” ë° ì„ê³„ê°’ ì¡°ì •
def search_similar_titles(user_input: str, max_results: int = 5):
    """ê°„ë‹¨í•˜ê³  ì •í™•í•œ ê²€ìƒ‰ í•¨ìˆ˜"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{user_input}'")
    
    # DB ì´ˆê¸°í™”
    if not _DB_INITIALIZED:
        print("ğŸ”„ DB ì´ˆê¸°í™” ì¤‘...")
        initialize_db()
    
    if _PROCESSED_DB is None:
        print("âš ï¸ ì „ì—­ DB ì—†ìŒ, ì§ì ‘ ë¡œë“œ ì‹œë„...")
        df = pd.read_excel(DB_PATH)
    else:
        df = _PROCESSED_DB
    
    if df.empty:
        print("âŒ DBê°€ ë¹„ì–´ìˆìŒ")
        return []
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë³€í™˜
    print("ğŸ“ 1ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë³€í™˜")
    keywords = extract_and_translate_keywords(user_input)
    if not keywords:
        print("âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        return []
    
    search_query = " ".join(keywords)
    print(f"ğŸ¯ ìµœì¢… ê²€ìƒ‰ì–´: '{search_query}'")
    
    # 2. ìœ ì‚¬ë„ ê³„ì‚°
    print("ğŸ”¢ 2ë‹¨ê³„: ìœ ì‚¬ë„ ê³„ì‚°")
    try:
        if _VECTORIZER is not None and _TFIDF_MATRIX is not None:
            search_vector = _VECTORIZER.transform([search_query])
            cosine_sim = cosine_similarity(search_vector, _TFIDF_MATRIX)[0]
            print(f"   âœ… ì‚¬ì „ ê³„ì‚°ëœ ë²¡í„° ì‚¬ìš©")
        else:
            # ìƒˆë¡œ ê³„ì‚°
            print("   âš ï¸ ìƒˆë¡œ ë²¡í„° ê³„ì‚° ì¤‘...")
            corpus = df['Project Title'].fillna("").astype(str).tolist()
            corpus.append(search_query)
            
            vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), lowercase=True)
            tfidf_matrix = vectorizer.fit_transform(corpus)
            cosine_sim = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])[0]
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []
    
    # 3. ê²°ê³¼ ì •ë ¬ ë° í•„í„°ë§
    print("ğŸ“Š 3ë‹¨ê³„: ê²°ê³¼ ë¶„ì„")
    result_df = df.copy()
    result_df['score'] = cosine_sim
    
    # ğŸ”¥ ìƒìœ„ ê²°ê³¼ í™•ì¸ ë¡œê·¸ ì¶”ê°€
    print(f"ğŸ”¢ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ, ìƒìœ„ 10ê°œ ê²°ê³¼:")
    top_10 = result_df.nlargest(10, 'score')[['Project Title', 'Category', 'score']]
    for idx, row in top_10.iterrows():
        print(f"  {row['score']:.6f}: [{row.get('Category', 'N/A')}] {row['Project Title'][:50]}...")
    
    # ğŸ”¥ ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì ì§„ì  ì‹œë„
    thresholds = [0.05, 0.02, 0.01, 0.005, 0.001]  # ë” ë‚®ì€ ì„ê³„ê°’ ì¶”ê°€
    filtered_df = None
    
    for threshold in thresholds:
        filtered_df = result_df[result_df['score'] > threshold]
        result_count = len(filtered_df)
        print(f"   ì„ê³„ê°’ {threshold}: {result_count}ê°œ ê²°ê³¼")
        
        # ì ë‹¹í•œ ìˆ˜ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
        if 1 <= result_count <= 20:  # ë²”ìœ„ í™•ëŒ€ (15â†’20)
            print(f"   âœ… ì„ê³„ê°’ {threshold} ì„ íƒ ({result_count}ê°œ)")
            break
    
    if filtered_df is None or filtered_df.empty:
        print("âŒ ê´€ë ¨ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return []
    
    # 4. ìƒìœ„ ê²°ê³¼ë§Œ ì„ íƒ (ìµœëŒ€ 5ê°œ)
    top_df = filtered_df.sort_values(by='score', ascending=False).head(max_results)
    
    print(f"ğŸ“‹ ì„ íƒëœ ê²°ê³¼ ({len(top_df)}ê°œ):")
    for idx, row in top_df.iterrows():
        print(f"  {row['score']:.4f}: [{row.get('Category', 'N/A')}] {row['Project Title'][:60]}...")
    
    # 5. ê²°ê³¼ êµ¬ì„± - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    print("ğŸ—ï¸ 4ë‹¨ê³„: ê²°ê³¼ êµ¬ì„± ë° ìš”ì•½ ìƒì„±")
    results = []
    for i, (_, row) in enumerate(top_df.iterrows()):
        try:
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            summary = generate_simple_summary(
                row.get('Project Title', ''), 
                row.get('Category', ''),
                i + 1
            )
        except Exception as e:
            print(f"âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ({i+1}ë²ˆ): {e}")
            summary = f"ì´ í”„ë¡œì íŠ¸ëŠ” '{row.get('Project Title', '')}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
        
        result_item = {
            'ì œëª©': row.get('Project Title', ''),
            'ì—°ë„': str(row.get('Year', '')),
            'ë¶„ì•¼': row.get('Category', ''),
            'êµ­ê°€': row.get('Fair Country', ''),
            'ì§€ì—­': row.get('Fair State', ''),
            'ìˆ˜ìƒ': row.get('Awards', ''),
            'ìš”ì•½': summary,
            'score': float(row.get('score', 0))
        }
        results.append(result_item)
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
    return results

# ë‚´ë¶€ DB ë¡œë“œ í•¨ìˆ˜ (í´ë°±ìš©)
@st.cache_data(ttl=3600)
def load_internal_db():
    """ê¸°ë³¸ DB ë¡œë“œ í•¨ìˆ˜"""
    try:
        df = pd.read_excel(DB_PATH)
        return df
    except Exception as e:
        st.error(f"âŒ ë‚´ë¶€ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ğŸ§ª í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¶”ê°€
def test_search():
    """ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    test_queries = ["ìš´ë™", "exercise", "ì²´ì§€ë°©", "í™˜ê²½", "ìš´ë™ê³¼ ì²´ì§€ë°© ê°ëŸ‰"]
    for query in test_queries:
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: '{query}'")
        try:
            results = search_similar_titles(query, max_results=3)
            print(f"âœ… ê²°ê³¼: {len(results)}ê°œ")
            if results:
                for i, result in enumerate(results[:2]):  # ìƒìœ„ 2ê°œë§Œ ì¶œë ¥
                    print(f"  {i+1})ì ìˆ˜:{result['score']:.4f} - {result['ì œëª©'][:50]}...")
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("-" * 50)

# ì‚¬ìš© ì˜ˆì‹œ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    print("ğŸš€ ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    test_search()
