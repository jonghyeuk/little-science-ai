import pandas as pd
import os
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from utils.explain_topic import explain_topic
from openai import OpenAI
import re

# ğŸ“ ë‚´ë¶€ DB ê²½ë¡œ
DB_PATH = os.path.join("data", "ISEF Final DB.xlsx")

# ğŸ”¤ ISEF DB ì—´ ë§¤í•‘
COLUMN_MAP = {
    'Project Title': 'ì œëª©',
    'Year': 'ì—°ë„',
    'Category': 'ë¶„ì•¼',
    'Fair Country': 'êµ­ê°€',
    'Fair State': 'ì§€ì—­',
    'Awards': 'ìˆ˜ìƒ'
}

# ì „ì—­ ë³€ìˆ˜ - ì‚¬ì „ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
_DB_INITIALIZED = False
_PROCESSED_DB = None
_VECTORIZER = None
_TFIDF_MATRIX = None

# ì´ˆê¸°í™” í•¨ìˆ˜ - ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
@st.cache_data(ttl=86400, show_spinner=False)  # ìºì‹œ 24ì‹œê°„ ìœ ì§€
def initialize_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    try:
        # Excel íŒŒì¼ ë¡œë“œ
        df = pd.read_excel(DB_PATH)
        _PROCESSED_DB = df
        
        # ë²¡í„°í™” ë¯¸ë¦¬ ìˆ˜í–‰
        title_field = 'Project Title'
        corpus = df[title_field].fillna("").astype(str).tolist()
        
        _VECTORIZER = TfidfVectorizer(
            analyzer='word', 
            ngram_range=(1, 2),
            lowercase=True
        )
        
        _TFIDF_MATRIX = _VECTORIZER.fit_transform(corpus)
        _DB_INITIALIZED = True
        
        print(f"âœ… ë‚´ë¶€ DB ì´ˆê¸°í™” ì™„ë£Œ: {len(df)} ê°œ ë…¼ë¬¸ ë¡œë“œë¨")
        return True
    except Exception as e:
        print(f"âŒ ë‚´ë¶€ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, top_n=5):
    text = re.sub(r'[^\w\s]', '', text.lower())
    stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë°', 'ë“±', 'ë¥¼', 'ì„', 'ì—', 'ì—ì„œ', 'ì˜', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ê²Œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                'the', 'of', 'and', 'a', 'to', 'in', 'is', 'that', 'for', 'on', 'with']
    
    words = [word for word in text.split() if word not in stopwords and len(word) > 1]
    
    word_counts = {}
    for word in words:
        word_counts[word] = word_counts.get(word, 0) + 1
    
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    return [word for word, _ in sorted_words[:top_n]]

# íš¨ìœ¨ì ì¸ GPT ë²ˆì—­ í•¨ìˆ˜ - í‚¤ì›Œë“œë§Œ ë²ˆì—­
@st.cache_data(show_spinner=False, ttl=3600)
def gpt_translate_keywords(keywords, tgt_lang="en") -> list:
    if not keywords:
        return []
        
    try:
        client = OpenAI(api_key=st.secrets["api"]["openai_key"])
        keyword_text = ", ".join(keywords)
        prompt = f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ {tgt_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”: {keyword_text}"
        
        res = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        
        translated = res.choices[0].message.content.strip()
        return [k.strip() for k in translated.split(',')]
    except Exception as e:
        st.warning(f"í‚¤ì›Œë“œ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}")
        return keywords

# ì¼ê´„ ì²˜ë¦¬ í•¨ìˆ˜ - ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì œëª© í•œ ë²ˆì— ì²˜ë¦¬
@st.cache_data(show_spinner=False, ttl=3600)
def batch_infer_content(titles, categories=None):
    """ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì œëª©ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ GPT API í˜¸ì¶œ ìµœì†Œí™”"""
    if not titles:
        return []
        
    try:
        client = OpenAI(api_key=st.secrets["api"]["openai_key"])
        
        # ë²”ì£¼ ì •ë³´ ì¶”ê°€
        prompts = []
        for i, title in enumerate(titles):
            category = categories[i] if categories and i < len(categories) else None
            if category:
                prompts.append(f"{i+1}. '{title}' (ì—°êµ¬ ë¶„ì•¼: {category})")
            else:
                prompts.append(f"{i+1}. '{title}'")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
        system_prompt = """
        ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë§Œìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ê·œì¹™:
        
        1. ì œëª©ë§Œ ìˆê³  ì‹¤ì œ ë…¼ë¬¸ì„ ë³´ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ëª…í™•íˆ ì–¸ê¸‰í•  ê²ƒ
        2. ëª¨ë“  ì„œìˆ ì€ "~ë¡œ ì¶”ì¸¡ë©ë‹ˆë‹¤", "~ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤" ê°™ì€ ì¶”ì¸¡í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        3. ê° ë…¼ë¬¸ì— ëŒ€í•´ 2-3ë¬¸ì¥ì˜ ê°„ê²°í•œ ì„¤ëª… ì œê³µ
        4. ê° ì‘ë‹µ ì‹œì‘ì— ë²ˆí˜¸ë¥¼ ìœ ì§€í•  ê²ƒ (1., 2., ë“±)
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê° ë…¼ë¬¸ì„ ì„¤ëª…í•˜ì„¸ìš”:
        "X. ì´ ì—°êµ¬ëŠ” [ì£¼ì œ]ì— ê´€í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì œëª©ì—ì„œ ìœ ì¶”í•˜ë©´, [ì˜ˆìƒ ëª©ì /ë°©ë²•]ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. (ì£¼ì˜: ì´ëŠ” ì œëª©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ë¡ ìœ¼ë¡œ, ì‹¤ì œ ì—°êµ¬ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
        """
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì‘ì„±
        user_prompt = "ë‹¤ìŒ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë“¤ì˜ ë‚´ìš©ì„ ì¶”ë¡ í•´ì£¼ì„¸ìš”:\n\n" + "\n".join(prompts)
        
        # API í˜¸ì¶œ
        res = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        
        full_response = res.choices[0].message.content.strip()
        
        # ì‘ë‹µ íŒŒì‹±
        summaries = []
        current_summary = ""
        current_index = 0
        
        lines = full_response.split('\n')
        for line in lines:
            # ìƒˆë¡œìš´ í•­ëª© ì‹œì‘ ê°ì§€ (1., 2., ë“±)
            match = re.match(r'^(\d+)\.', line)
            if match:
                # ì´ì „ ìš”ì•½ ì €ì¥
                if current_summary and current_index > 0:
                    summaries.append(current_summary.strip())
                    
                # ìƒˆ ìš”ì•½ ì‹œì‘
                index = int(match.group(1))
                current_index = index
                current_summary = line
            elif current_summary:  # í˜„ì¬ ìš”ì•½ì— ë¼ì¸ ì¶”ê°€
                current_summary += " " + line
        
        # ë§ˆì§€ë§‰ ìš”ì•½ ì €ì¥
        if current_summary:
            summaries.append(current_summary.strip())
        
        # ìš”ì•½ ê°œìˆ˜ í™•ì¸ ë° ì¡°ì • 
        if len(summaries) < len(titles):
            # ë¶€ì¡±í•œ ìš”ì•½ ì¶”ê°€
            for i in range(len(summaries), len(titles)):
                title = titles[i]
                summaries.append(f"{i+1}. ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ìš”ì•½ ìƒì„± ì‹¤íŒ¨)")
        
        return summaries
    
    except Exception as e:
        print(f"ì¼ê´„ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ìš”ì•½ ë°˜í™˜
        return [f"ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ì¶”ë¡  ì˜¤ë¥˜)" for title in titles]

# ê°œë³„ í”„ë¡œì íŠ¸ ë‚´ìš© ì¶”ë¡  í•¨ìˆ˜ (ê¸°ì¡´ í•¨ìˆ˜ ë³´ì¡´)
@st.cache_data(show_spinner=False, ttl=3600)
def infer_project_content(title, category=None):
    try:
        client = OpenAI(api_key=st.secrets["api"]["openai_key"])
        
        system_prompt = """
        ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë§Œìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ë¡ í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ê·œì¹™:
        
        1. ì œëª©ë§Œ ìˆê³  ì‹¤ì œ ë…¼ë¬¸ì„ ë³´ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ëª…í™•íˆ ì–¸ê¸‰í•  ê²ƒ
        2. ëª¨ë“  ì„œìˆ ì€ "~ë¡œ ì¶”ì¸¡ë©ë‹ˆë‹¤", "~ì„ ë‹¤ë£¨ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤" ê°™ì€ ì¶”ì¸¡í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        3. êµ¬ì²´ì ì¸ ë°©ë²•ë¡ ê³¼ ê²°ê³¼ëŠ” ì™„ì „íˆ ì¶”ì¸¡ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹œí•  ê²ƒ
        4. ì œëª©ì—ì„œ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ì—°êµ¬ ì£¼ì œì™€ ì˜ˆìƒë˜ëŠ” ì ‘ê·¼ë²• ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•  ê²ƒ
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
        "ì´ ì—°êµ¬ëŠ” [ì£¼ì œ]ì— ê´€í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì œëª©ì—ì„œ ìœ ì¶”í•˜ë©´, [ì˜ˆìƒ ëª©ì ]ì„ ìœ„í•´ [ì˜ˆìƒ ë°©ë²•]ì„ ì‚¬ìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” [ì˜ˆìƒ ë¶„ì•¼]ì— ê¸°ì—¬í•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, [ì ì¬ì  ì˜ì˜]ë¥¼ ê°€ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. (ì£¼ì˜: ì´ëŠ” ì œëª©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ë¡ ìœ¼ë¡œ, ì‹¤ì œ ì—°êµ¬ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
        """
        
        user_prompt = title
        if category:
            user_prompt = f"{title} (ì—°êµ¬ ë¶„ì•¼: {category})"
            
        res = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        
        return res.choices[0].message.content.strip()
    except Exception as e:
        return f"ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ì¶”ë¡  ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.)"

# ë‚´ë¶€ DB ë¡œë“œ ë° ì •ì œ (ê¸°ì¡´ í•¨ìˆ˜ ë³´ì¡´)
@st.cache_data(ttl=3600)
def load_internal_db():
    try:
        df = pd.read_excel(DB_PATH)
        return df
    except Exception as e:
        st.error(f"âŒ ë‚´ë¶€ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ìµœì í™”ëœ ìœ ì‚¬ í”„ë¡œì íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_similar_titles(user_input, max_results=5):
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    # DB ì´ˆê¸°í™” í™•ì¸ ë° ìˆ˜í–‰
    if not _DB_INITIALIZED:
        initialize_db()
    
    # ê¸°ë³¸ DB ì‚¬ìš© (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)
    if _PROCESSED_DB is None:
        df = load_internal_db()
    else:
        df = _PROCESSED_DB
    
    if df.empty:
        return []
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(user_input)
    if not keywords:
        return []
    
    # 2. í‚¤ì›Œë“œ ë²ˆì—­ (API í˜¸ì¶œ 1íšŒ)
    translated_keywords = gpt_translate_keywords(keywords)
    if not translated_keywords:
        translated_keywords = keywords
    
    search_query = " ".join(translated_keywords)
    
    # 3. ì˜ì–´ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰ (ì´ˆê¸°í™”ëœ ë²¡í„°ë¼ì´ì € ì‚¬ìš©)
    try:
        if _VECTORIZER and _TFIDF_MATRIX:
            # ì‚¬ì „ ì²˜ë¦¬ëœ ë²¡í„°ë¼ì´ì €ì™€ ë§¤íŠ¸ë¦­ìŠ¤ ì‚¬ìš©
            search_vector = _VECTORIZER.transform([search_query])
            cosine_sim = cosine_similarity(search_vector, _TFIDF_MATRIX)[0]
        else:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            title_field = 'Project Title'
            corpus = df[title_field].fillna("").astype(str).tolist()
            corpus.append(search_query)
            
            vectorizer = TfidfVectorizer(
                analyzer='word', 
                ngram_range=(1, 2),
                lowercase=True
            )
            
            tfidf_matrix = vectorizer.fit_transform(corpus)
            cosine_sim = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])[0]
    except Exception as e:
        print(f"ê²€ìƒ‰ ë²¡í„°í™” ì˜¤ë¥˜: {e}")
        return []
    
    # 4. ê²°ê³¼ ì •ë ¬
    result_df = df.copy()
    result_df['score'] = cosine_sim
    filtered_df = result_df[result_df['score'] > 0.05].copy()
    
    if filtered_df.empty:
        return []
    
    # 5. ìƒìœ„ ê²°ê³¼ ì„ íƒ
    top_df = filtered_df.sort_values(by='score', ascending=False).head(max_results)
    
    # 6. ê²°ê³¼ë¥¼ ìœ„í•œ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
    titles = []
    categories = []
    for _, row in top_df.iterrows():
        titles.append(row.get('Project Title', ''))
        categories.append(row.get('Category', ''))
    
    # 7. ì¼ê´„ ì²˜ë¦¬ë¡œ ëª¨ë“  ìš”ì•½ í•œ ë²ˆì— ìƒì„±
    if titles:
        summaries = batch_infer_content(titles, categories)
    else:
        summaries = []
    
    # 8. ê²°ê³¼ êµ¬ì„±
    results = []
    for i, (_, row) in enumerate(top_df.iterrows()):
        project_title = row.get('Project Title', '')
        category = row.get('Category', '')
        
        # ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        summary = ""
        if i < len(summaries):
            # ìš”ì•½ì—ì„œ ë²ˆí˜¸ ì œê±°
            summary_text = summaries[i]
            summary = re.sub(r'^\d+\.\s*', '', summary_text)
        else:
            summary = f"ì´ í”„ë¡œì íŠ¸ëŠ” '{project_title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
        
        result_item = {
            'ì œëª©': project_title,  # ì˜ì–´ ì œëª© ì‚¬ìš©
            'ì—°ë„': str(row.get('Year', '')),
            'ë¶„ì•¼': category,
            'êµ­ê°€': row.get('Fair Country', ''),
            'ì§€ì—­': row.get('Fair State', ''),
            'ìˆ˜ìƒ': row.get('Awards', ''),
            'ìš”ì•½': summary,
            'score': float(row.get('score', 0))
        }
        
        results.append(result_item)
    
    return results
