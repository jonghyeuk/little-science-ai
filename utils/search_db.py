# ìˆ˜ì •ëœ search_db.py - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€ ë° ì¤‘ë³µ ì œê±°

import pandas as pd
import os
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import anthropic
import re

# ğŸ“ ë‚´ë¶€ DB ê²½ë¡œ
DB_PATH = os.path.join("data", "ISEF Final DB.xlsx")

# ì „ì—­ ë³€ìˆ˜ - ì‚¬ì „ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
_DB_INITIALIZED = False
_PROCESSED_DB = None
_VECTORIZER = None
_TFIDF_MATRIX = None

# ì´ˆê¸°í™” í•¨ìˆ˜ - ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
@st.cache_data(ttl=86400, show_spinner=False)
def initialize_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    try:
        # Excel íŒŒì¼ ë¡œë“œ
        df = pd.read_excel(DB_PATH)
        _PROCESSED_DB = df
        
        # ë²¡í„°í™” ë¯¸ë¦¬ ìˆ˜í–‰
        title_field = 'Project Title'
        corpus = df[title_field].fillna("").astype(str).tolist()
        
        _VECTORIZER = TfidfVectorizer(
            analyzer='word', 
            ngram_range=(1, 2),
            lowercase=True
        )
        
        _TFIDF_MATRIX = _VECTORIZER.fit_transform(corpus)
        _DB_INITIALIZED = True
        
        print(f"âœ… ë‚´ë¶€ DB ì´ˆê¸°í™” ì™„ë£Œ: {len(df)} ê°œ ë…¼ë¬¸ ë¡œë“œë¨")
        return True
    except Exception as e:
        print(f"âŒ ë‚´ë¶€ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, top_n=5):
    """í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    text = re.sub(r'[^\w\s]', '', text.lower())
    
    stopwords = [
        'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì˜', 'ëŠ”', 'ì€', 'ë„', 'ë§Œ', 
        'ë¶€í„°', 'ê¹Œì§€', 'ì™€', 'ê³¼', 'í•˜ê³ ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
        'the', 'of', 'and', 'a', 'to', 'in', 'is', 'that', 'for', 'on', 'with', 'as', 
        'be', 'by', 'from', 'at', 'or'
    ]
    
    words = []
    for word in text.split():
        if len(word) >= 2 and word not in stopwords and not word.isdigit():
            # í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°
            if any('\uAC00' <= char <= '\uD7A3' for char in word):
                if word.endswith(('ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ë¡œ', 'ì˜', 'ëŠ”', 'ì€')):
                    word = word[:-1]
                elif word.endswith(('ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'í•œí…Œ')):
                    word = word[:-2]
            words.append(word)
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_counts = {}
    for word in words:
        word_counts[word] = word_counts.get(word, 0) + 1
    
    # ë¹ˆë„ìˆœ ì •ë ¬
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    top_keywords = [word for word, _ in sorted_words[:top_n]]
    
    return top_keywords

# â­ ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€: í‚¤ì›Œë“œ ë²ˆì—­ í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=3600)
def claude_translate_keywords(keywords, tgt_lang="en"):
    """í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    if not keywords:
        return []
        
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        keyword_text = ", ".join(keywords)
        
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=100,
            system=f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ {tgt_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”.",
            messages=[
                {"role": "user", "content": keyword_text}
            ]
        )
        
        translated = response.content[0].text.strip()
        return [k.strip() for k in translated.split(',')]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}")
        return keywords

# ì¼ê´„ ì²˜ë¦¬ í•¨ìˆ˜ - Claude API íš¨ìœ¨í™”
@st.cache_data(show_spinner=False, ttl=3600)
def batch_infer_content(titles, categories=None):
    """ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì œëª©ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ Claude API í˜¸ì¶œ ìµœì†Œí™”"""
    if not titles:
        return []
    
    print(f"=== ì¼ê´„ ìš”ì•½ ìƒì„± ì‹œì‘: {len(titles)}ê°œ ë…¼ë¬¸ ===")    
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        
        # ì œëª©ì´ ë„ˆë¬´ ë§ìœ¼ë©´ 5ê°œì”© ë¶„í•  ì²˜ë¦¬
        if len(titles) > 5:
            print(f"âš ï¸ ì œëª©ì´ {len(titles)}ê°œë¡œ ë§ìŒ, 5ê°œì”© ë¶„í•  ì²˜ë¦¬")
            all_summaries = []
            for i in range(0, len(titles), 5):
                batch_titles = titles[i:i+5]
                batch_categories = categories[i:i+5] if categories else None
                batch_summaries = batch_infer_content(batch_titles, batch_categories)
                all_summaries.extend(batch_summaries)
            return all_summaries
        
        # ë²”ì£¼ ì •ë³´ ì¶”ê°€
        prompts = []
        for i, title in enumerate(titles):
            category = categories[i] if categories and i < len(categories) else None
            if category:
                prompts.append(f"{i+1}. '{title}' (ë¶„ì•¼: {category})")
            else:
                prompts.append(f"{i+1}. '{title}'")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """
        ê³¼í•™ ë…¼ë¬¸ ì œëª©ì„ ë³´ê³  ë‚´ìš©ì„ ì¶”ë¡ í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        
        ê·œì¹™:
        1. ê° ë…¼ë¬¸ë§ˆë‹¤ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ì„œ ì„¤ëª… (1., 2., 3. ...)
        2. ì œëª©ë§Œ ë³´ê³  ì¶”ë¡ í•˜ë¯€ë¡œ "~ë¡œ ì¶”ì •ë©ë‹ˆë‹¤", "~ì¼ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤" í‘œí˜„ ì‚¬ìš©
        3. ê° ì„¤ëª…ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
        4. ì „ë¬¸ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
        
        í˜•ì‹ ì˜ˆì‹œ:
        1. ì´ ì—°êµ¬ëŠ” [ì£¼ì œ]ì— ê´€í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. [ì˜ˆìƒ ë‚´ìš© ì„¤ëª…]. ì´ëŠ” [ì˜ì˜/ì‘ìš©ë¶„ì•¼]ì— ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
        """
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì‘ì„±
        user_prompt = "ë‹¤ìŒ ê³¼í•™ ë…¼ë¬¸ ì œëª©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n" + "\n".join(prompts)
        
        # Claude API í˜¸ì¶œ
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1500,
            temperature=0.5,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        full_response = response.content[0].text.strip()
        print(f"âœ… Claude ì‘ë‹µ ë°›ìŒ: {len(full_response)} ê¸€ì")
        
        # ì‘ë‹µ íŒŒì‹±
        summaries = []
        lines = full_response.split('\n')
        current_summary = ""
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ê°ì§€ (1., 2., 3. ë“±)
            number_match = re.match(r'^(\d+)\.', line)
            if number_match:
                # ì´ì „ ìš”ì•½ ì €ì¥
                if current_summary:
                    summaries.append(current_summary.strip())
                # ìƒˆ ìš”ì•½ ì‹œì‘
                current_summary = line
            else:
                # í˜„ì¬ ìš”ì•½ì— ë¼ì¸ ì¶”ê°€
                if current_summary:
                    current_summary += " " + line
        
        # ë§ˆì§€ë§‰ ìš”ì•½ ì €ì¥
        if current_summary:
            summaries.append(current_summary.strip())
        
        # ë¶€ì¡±í•œ ìš”ì•½ ë³´ì¶©
        while len(summaries) < len(titles):
            idx = len(summaries)
            title = titles[idx] if idx < len(titles) else "Unknown"
            fallback_summary = f"{idx+1}. ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
            summaries.append(fallback_summary)
        
        # ì´ˆê³¼ ìš”ì•½ ì œê±°
        if len(summaries) > len(titles):
            summaries = summaries[:len(titles)]
        
        print(f"âœ… ì¼ê´„ ìš”ì•½ ì™„ë£Œ: {len(summaries)}ê°œ")
        return summaries
    
    except Exception as e:
        print(f"âŒ ì¼ê´„ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ í´ë°±
        return [f"{i+1}. ì´ í”„ë¡œì íŠ¸ëŠ” '{title}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. (ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ)" 
                for i, title in enumerate(titles)]

# ë‚´ë¶€ DB ë¡œë“œ í•¨ìˆ˜
@st.cache_data(ttl=3600)
def load_internal_db():
    """ê¸°ë³¸ DB ë¡œë“œ í•¨ìˆ˜"""
    try:
        df = pd.read_excel(DB_PATH)
        return df
    except Exception as e:
        st.error(f"âŒ ë‚´ë¶€ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_similar_titles(user_input: str, max_results: int = 10):
    """ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ - ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ìœ ì‚¬í•œ ë…¼ë¬¸ ì œëª©ë“¤ì„ ê²€ìƒ‰"""
    global _DB_INITIALIZED, _PROCESSED_DB, _VECTORIZER, _TFIDF_MATRIX
    
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{user_input}'")
    
    # DB ì´ˆê¸°í™” í™•ì¸
    if not _DB_INITIALIZED:
        initialize_db()
    
    # DB ë¡œë“œ
    if _PROCESSED_DB is None:
        df = load_internal_db()
    else:
        df = _PROCESSED_DB
    
    if df.empty:
        print("âŒ DBê°€ ë¹„ì–´ìˆìŒ")
        return []
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ“ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    keywords = extract_keywords(user_input)
    print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
    
    if not keywords:
        print("âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        return []
    
    # 2. í‚¤ì›Œë“œ ë²ˆì—­
    print("ğŸŒ í‚¤ì›Œë“œ ë²ˆì—­ ì¤‘...")
    translated_keywords = claude_translate_keywords(keywords)
    print(f"   ë²ˆì—­ëœ í‚¤ì›Œë“œ: {translated_keywords}")
    
    if not translated_keywords:
        translated_keywords = keywords
    
    search_query = " ".join(translated_keywords)
    print(f"   ìµœì¢… ê²€ìƒ‰ì–´: '{search_query}'")
    
    # 3. ìœ ì‚¬ë„ ê³„ì‚°
    print("ğŸ”¢ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    try:
        if _VECTORIZER is not None and _TFIDF_MATRIX is not None:
            # ì‚¬ì „ ì²˜ë¦¬ëœ ë²¡í„° ì‚¬ìš©
            search_vector = _VECTORIZER.transform([search_query])
            cosine_sim = cosine_similarity(search_vector, _TFIDF_MATRIX)[0]
            print(f"   ë²¡í„°í™” ì„±ê³µ, {len(cosine_sim)}ê°œ ë¬¸ì„œì™€ ë¹„êµ")
        else:
            # ìƒˆë¡œ ê³„ì‚°
            print("   ìƒˆë¡œ ë²¡í„°í™” ê³„ì‚°...")
            title_field = 'Project Title'
            corpus = df[title_field].fillna("").astype(str).tolist()
            corpus.append(search_query)
            
            vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), lowercase=True)
            tfidf_matrix = vectorizer.fit_transform(corpus)
            cosine_sim = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])[0]
            print(f"   ìƒˆë¡œ ë²¡í„°í™” ì™„ë£Œ, {len(cosine_sim)}ê°œ ë¬¸ì„œì™€ ë¹„êµ")
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ë²¡í„°í™” ì˜¤ë¥˜: {e}")
        return []
    
    # 4. ê²°ê³¼ ì •ë ¬ ë° í•„í„°ë§
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
    result_df = df.copy()
    result_df['score'] = cosine_sim
    
    # ìƒìœ„ ê²°ê³¼ í™•ì¸
    top_scores = result_df.nlargest(10, 'score')[['Project Title', 'Category', 'score']]
    print("   ìƒìœ„ 10ê°œ ìœ ì‚¬ë„ ì ìˆ˜:")
    for idx, row in top_scores.iterrows():
        print(f"     {row['score']:.6f}: [{row.get('Category', 'N/A')}] {row['Project Title'][:60]}...")
    
    # ì„ê³„ê°’ ì„¤ì •
    threshold = 0.005
    filtered_df = result_df[result_df['score'] > threshold].copy()
    
    if filtered_df.empty:
        print("âŒ ê´€ë ¨ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return []
    
    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
    top_df = filtered_df.sort_values(by='score', ascending=False).head(max_results)
    print(f"   ìµœì¢… ì„ íƒ: {len(top_df)}ê°œ")
    
    # 5. AI ìš”ì•½ ìƒì„±
    titles = [row.get('Project Title', '') for _, row in top_df.iterrows()]
    categories = [row.get('Category', '') for _, row in top_df.iterrows()]
    
    if titles:
        print("ğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
        summaries = batch_infer_content(titles, categories)
        print(f"   ìƒì„±ëœ ìš”ì•½: {len(summaries)}ê°œ")
    else:
        summaries = []
    
    # 6. ê²°ê³¼ êµ¬ì„±
    results = []
    for i, (_, row) in enumerate(top_df.iterrows()):
        summary = ""
        if i < len(summaries):
            summary_text = summaries[i]
            summary = re.sub(r'^\d+\.\s*', '', summary_text)
        else:
            summary = f"ì´ í”„ë¡œì íŠ¸ëŠ” '{row.get('Project Title', '')}'ì— ê´€í•œ ì—°êµ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
        
        result_item = {
            'ì œëª©': row.get('Project Title', ''),
            'ì—°ë„': str(row.get('Year', '')),
            'ë¶„ì•¼': row.get('Category', ''),
            'êµ­ê°€': row.get('Fair Country', ''),
            'ì§€ì—­': row.get('Fair State', ''),
            'ìˆ˜ìƒ': row.get('Awards', ''),
            'ìš”ì•½': summary,
            'score': float(row.get('score', 0))
        }
        results.append(result_item)
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
    return results
