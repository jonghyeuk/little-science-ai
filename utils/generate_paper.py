# utils/generate_paper.py
import streamlit as st
import anthropic
import json
import re

@st.cache_data(ttl=3600, show_spinner=False)
def generate_research_paper(topic, research_idea, references=""):
    """
    ì„ íƒëœ ì—°êµ¬ ì•„ì´ë””ì–´ì— ëŒ€í•œ ë…¼ë¬¸ í˜•ì‹ì˜ ì—°êµ¬ ê³„íšì„ ìƒì„±
    """
    try:
        client = anthropic.Anthropic(api_key=st.secrets["api"]["claude_key"])
        
        # ğŸ”¥ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ìˆ˜ì •
        system_prompt = """
        ê³ ë“±í•™ìƒ ì—°êµ¬ ê³„íšì„œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ë§Œ ì‚¬ìš©:
        {"abstract": "ì´ˆë¡ë‚´ìš©", "introduction": "ì„œë¡ ë‚´ìš©", "methods": "ë°©ë²•ë‚´ìš©", "results": "ê²°ê³¼ë‚´ìš©", "visuals": "ì‹œê°ìë£Œë‚´ìš©", "conclusion": "ê²°ë¡ ë‚´ìš©", "references": "ì°¸ê³ ë¬¸í—Œë‚´ìš©"}

        ê° ì„¹ì…˜ ìš”êµ¬ì‚¬í•­:
        - abstract: ì—°êµ¬ëª©ì ê³¼ ì˜ˆìƒê²°ê³¼ ìš”ì•½ (100-150ë‹¨ì–´)
        - introduction: ë°°ê²½â†’ë¬¸ì œâ†’ëª©ì  ìˆœì„œ (200-250ë‹¨ì–´)
        - methods: ì‹¤í—˜ë‹¨ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•´ì„œ ì‘ì„± (250-300ë‹¨ì–´)
        - results: ì˜ˆìƒë˜ëŠ” êµ¬ì²´ì  ê²°ê³¼ë“¤ (150-200ë‹¨ì–´)
        - visuals: í•„ìš”í•œ ê·¸ë˜í”„/ì°¨íŠ¸ ì„¤ëª… (100-150ë‹¨ì–´)
        - conclusion: ì—°êµ¬ì˜ ì˜ì˜ì™€ í™œìš©ë°©ì•ˆ (100-150ë‹¨ì–´)
        - references: ì‹¤ì œ í™•ì¸ê°€ëŠ¥í•œ ìë£Œ 3-4ê°œ

        ì‹¤í—˜ë°©ë²• ì‘ì„±ë²•:
        1ë‹¨ê³„: [ì œëª©] - êµ¬ì²´ì  ì„¤ëª…
        2ë‹¨ê³„: [ì œëª©] - êµ¬ì²´ì  ì„¤ëª…
        3ë‹¨ê³„: [ì œëª©] - êµ¬ì²´ì  ì„¤ëª…
        (ì´ëŸ° ì‹ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„)

        ì°¸ê³ ë¬¸í—Œ ì‘ì„±ë²•:
        1. ìë£Œì œëª©
        - ë‚´ìš©: í•µì‹¬ë‚´ìš© 2ë¬¸ì¥ ì„¤ëª…  
        - ê²€ìƒ‰: Google Scholarì—ì„œ "[í‚¤ì›Œë“œ]" ê²€ìƒ‰ ë˜ëŠ” ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì‚¬ì´íŠ¸ë§Œ (nasa.gov, nih.gov, ëŒ€í•™ì‚¬ì´íŠ¸ ë“±)
        - í™œìš©: ì—°êµ¬ì— ì–´ë–»ê²Œ ë„ì›€ë˜ëŠ”ì§€
        
        ì£¼ì˜: ì ˆëŒ€ë¡œ ê°€ì§œ DOIë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë§í¬ë¥¼ ë§Œë“¤ì§€ ë§ê³ , ëŒ€ì‹  ê²€ìƒ‰ ë°©ë²•ì„ ì•ˆë‚´í•˜ì„¸ìš”.
        """
        
        # ğŸ”¥ ê°„ë‹¨í•œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"""
        ì£¼ì œ: {topic}
        ì•„ì´ë””ì–´: {research_idea}

        ìœ„ ë‚´ìš©ìœ¼ë¡œ ê³ ë“±í•™ìƒ ì—°êµ¬ê³„íšì„œë¥¼ JSONìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        ì‹¤í—˜ë°©ë²•ì€ ë‹¨ê³„ë³„ë¡œ ëª…í™•íˆ, ë‹¤ë¥¸ ì„¹ì…˜ë“¤ë„ ê· í˜•ìˆê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        # Claude í˜¸ì¶œ
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2500,  # í† í° ì¤„ì—¬ì„œ ë” ì§‘ì¤‘ëœ ì‘ë‹µ
            temperature=0.2,  # ë” ì¼ê´€ëœ ì‘ë‹µ
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        response_text = response.content[0].text.strip()
        print(f"=== Claude ì‘ë‹µ ì›ë³¸ ===")
        print(response_text[:300] + "...")
        
        # ğŸ”¥ ë” ê°•ë ¥í•œ JSON ì¶”ì¶œ
        paper_data = extract_json_robust(response_text)
        
        # ğŸ”¥ ì„¹ì…˜ë³„ ê²€ì¦ ë° ìˆ˜ì •
        if paper_data:
            paper_data = validate_and_fix_sections(paper_data)
        
        return paper_data if paper_data else create_error_response()
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ë…¼ë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return create_error_response()

def extract_json_robust(text):
    """ë” ê°•ë ¥í•œ JSON ì¶”ì¶œ"""
    try:
        # ë°©ë²• 1: ì§ì ‘ íŒŒì‹±
        try:
            return json.loads(text)
        except:
            pass
        
        # ë°©ë²• 2: ì½”ë“œë¸”ë¡ ì œê±°
        if "```json" in text:
            content = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            content = text.split("```")[1].strip()
        else:
            content = text
        
        try:
            return json.loads(content)
        except:
            pass
        
        # ë°©ë²• 3: ì •ê·œì‹ìœ¼ë¡œ JSON ì°¾ê¸°
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        for match in matches:
            try:
                data = json.loads(match)
                if isinstance(data, dict) and 'abstract' in data:
                    return data
            except:
                continue
        
        # ë°©ë²• 4: ìˆ˜ë™ íŒŒì‹±
        return manual_parse_sections(text)
        
    except:
        return None

def manual_parse_sections(text):
    """ìˆ˜ë™ìœ¼ë¡œ ì„¹ì…˜ íŒŒì‹±"""
    try:
        sections = {
            "abstract": "",
            "introduction": "",
            "methods": "",
            "results": "",
            "visuals": "",
            "conclusion": "",
            "references": ""
        }
        
        # ë” ìœ ì—°í•œ í‚¤ì›Œë“œ ê²€ìƒ‰
        keywords = {
            'abstract': ['ì´ˆë¡', 'abstract', 'ìš”ì•½'],
            'introduction': ['ì„œë¡ ', 'introduction', 'ë°°ê²½', 'ë„ì…'],
            'methods': ['ë°©ë²•', 'method', 'ì‹¤í—˜', 'ì ˆì°¨'],
            'results': ['ê²°ê³¼', 'result', 'ì˜ˆìƒ'],
            'visuals': ['ì‹œê°', 'visual', 'ê·¸ë˜í”„', 'ì°¨íŠ¸'],
            'conclusion': ['ê²°ë¡ ', 'conclusion', 'ê²°ê³¼'],
            'references': ['ì°¸ê³ ', 'reference', 'ë¬¸í—Œ']
        }
        
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('{') or line.startswith('}'):
                continue
            
            # ì„¹ì…˜ ê°ì§€
            found_section = None
            for section, kws in keywords.items():
                if any(kw in line.lower() for kw in kws):
                    found_section = section
                    break
            
            if found_section:
                current_section = found_section
            elif current_section and not line.startswith('"') and not line.startswith(','):
                # ë‚´ìš© ì¶”ê°€
                if sections[current_section]:
                    sections[current_section] += " " + line
                else:
                    sections[current_section] = line
        
        # ë¹ˆ ì„¹ì…˜ ì²˜ë¦¬
        for key, value in sections.items():
            if not value.strip():
                sections[key] = get_default_content(key)
        
        return sections
        
    except:
        return None

def validate_and_fix_sections(paper_data):
    """ì„¹ì…˜ë³„ ê²€ì¦ ë° ìˆ˜ì •"""
    try:
        required_sections = ['abstract', 'introduction', 'methods', 'results', 'visuals', 'conclusion', 'references']
        
        for section in required_sections:
            if section not in paper_data or not paper_data[section] or len(paper_data[section].strip()) < 20:
                paper_data[section] = get_default_content(section)
        
        # methods ì„¹ì…˜ íŠ¹ë³„ ì²˜ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ë‹¨ì¶•)
        if len(paper_data['methods']) > 1000:
            methods_text = paper_data['methods'][:800] + "...\n\në°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„: ì‹¤í—˜ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡í•˜ê³  í†µê³„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
            paper_data['methods'] = methods_text
        
        # references ì„¹ì…˜ ì •ë¦¬
        if paper_data['references']:
            paper_data['references'] = clean_references(paper_data['references'])
        
        return paper_data
        
    except:
        return paper_data

def get_default_content(section):
    """ê¸°ë³¸ ë‚´ìš© ì œê³µ"""
    defaults = {
        'abstract': "ë³¸ ì—°êµ¬ëŠ” ì œì‹œëœ ì£¼ì œì— ëŒ€í•´ ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•´ ê³¼í•™ì  ê·¼ê±°ë¥¼ í™•ë³´í•˜ê³ ì í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ ì–»ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ìˆëŠ” ê²°ë¡ ì„ ë„ì¶œí•  ì˜ˆì •ì…ë‹ˆë‹¤.",
        'introduction': "í˜„ì¬ ê´€ë ¨ ë¶„ì•¼ì—ì„œëŠ” ë‹¤ì–‘í•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆì§€ë§Œ, ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ë²•ì„ ì œì‹œí•˜ê³ ì í•©ë‹ˆë‹¤.",
        'methods': "1ë‹¨ê³„: ì‹¤í—˜ ì¬ë£Œ ì¤€ë¹„\ní•„ìš”í•œ ì‹¤í—˜ ë„êµ¬ì™€ ì¬ë£Œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n\n2ë‹¨ê³„: ì‹¤í—˜ í™˜ê²½ ì„¤ì •\nì‹¤í—˜ì— ì í•©í•œ í™˜ê²½ì„ ì¡°ì„±í•©ë‹ˆë‹¤.\n\n3ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘\nì²´ê³„ì ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ë©° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n\n4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„\nìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ìˆëŠ” íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤.",
        'results': "ì‹¤í—˜ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤: ì¸¡ì •ê°’ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„, ê°€ì„¤ì˜ ê²€ì¦ ê²°ê³¼, ê·¸ë¦¬ê³  ì‹¤ìš©ì  í™œìš© ê°€ëŠ¥ì„±ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤.",
        'visuals': "ì‹¤í—˜ ê²°ê³¼ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì‹œê°ìë£Œë¥¼ ì œì‘í•  ì˜ˆì •ì…ë‹ˆë‹¤: ì‹¤í—˜ ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ì§„, ë°ì´í„° ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„, ê²°ê³¼ë¥¼ ìš”ì•½í•œ í‘œ ë“±ì…ë‹ˆë‹¤.",
        'conclusion': "ë³¸ ì—°êµ¬ë¥¼ í†µí•´ ì œì‹œëœ ì£¼ì œì— ëŒ€í•œ ìƒˆë¡œìš´ ì´í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì´ë©°, ì´ëŠ” ê´€ë ¨ ë¶„ì•¼ì˜ ë°œì „ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. ë˜í•œ ì‹¤ìƒí™œì—ì„œì˜ ì‘ìš© ê°€ëŠ¥ì„±ë„ íƒêµ¬í•  ì˜ˆì •ì…ë‹ˆë‹¤.",
        'references': "1. ê´€ë ¨ ì£¼ì œ ì—°êµ¬ ë™í–¥\n- ë‚´ìš©: í•´ë‹¹ ë¶„ì•¼ì˜ ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ ì£¼ìš” ë°œê²¬ì‚¬í•­ì„ ì •ë¦¬í•œ ìë£Œ\n- ê²€ìƒ‰: Google Scholarì—ì„œ 'ì£¼ì œëª… + research trends' ê²€ìƒ‰\n- í™œìš©: ì—°êµ¬ ë°°ê²½ ì´í•´ì™€ ë°©í–¥ ì„¤ì •ì— ë„ì›€\n\n2. ì‹¤í—˜ ë°©ë²•ë¡  ê°€ì´ë“œ\n- ë‚´ìš©: ê³¼í•™ì  ì‹¤í—˜ ì„¤ê³„ì™€ ë°ì´í„° ë¶„ì„ ë°©ë²•ì— ëŒ€í•œ ì¢…í•©ì  ì•ˆë‚´\n- ê²€ìƒ‰: ê° ëŒ€í•™êµ ê³¼í•™êµìœ¡ê³¼ ë˜ëŠ” ì‹¤í—˜ë°©ë²•ë¡  ê´€ë ¨ êµì¬ ê²€ìƒ‰\n- í™œìš©: ì²´ê³„ì ì¸ ì‹¤í—˜ ì§„í–‰ì„ ìœ„í•œ ì°¸ê³ ìë£Œ\n\n3. ì •ë¶€ ì—°êµ¬ ë³´ê³ ì„œ\n- ë‚´ìš©: ê´€ë ¨ ë¶„ì•¼ì— ëŒ€í•œ ì •ë¶€ ì°¨ì›ì˜ ì—°êµ¬ ë° ì •ì±… ìë£Œ\n- ê²€ìƒ‰: êµ­ê°€ê³¼í•™ê¸°ìˆ ì •ë³´ì„¼í„°(NDSL) ë˜ëŠ” ê´€ë ¨ ì •ë¶€ê¸°ê´€ í™ˆí˜ì´ì§€\n- í™œìš©: êµ­ê°€ì  ê´€ì ì—ì„œì˜ ì—°êµ¬ ë°©í–¥ì„± íŒŒì•…"
    }
    return defaults.get(section, f"{section} ì„¹ì…˜ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def clean_references(ref_text):
    """ì°¸ê³ ë¬¸í—Œ ì •ë¦¬ - ê°€ì§œ ë§í¬ ì œê±°í•˜ê³  ê²€ìƒ‰ ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´"""
    try:
        cleaned = ref_text
        
        # ğŸ”¥ ê°€ì§œ DOIì™€ ë§í¬ë“¤ ì œê±°
        fake_patterns = [
            r'https?://[^\s]*X\d+',  # X123456789 ê°™ì€ ê°€ì§œ ID
            r'https?://doi\.org/10\.\d+/[^\s]*XXX[^\s]*',  # XXX í¬í•¨ ê°€ì§œ DOI
            r'https?://[^\s]*fake[^\s]*',  # fake í¬í•¨ ë§í¬
            r'DOI:\s*10\.\d+/[^\s]*XXX[^\s]*'  # ê°€ì§œ DOI íŒ¨í„´
        ]
        
        for pattern in fake_patterns:
            cleaned = re.sub(pattern, '', cleaned)
        
        # ğŸ”¥ "ë§í¬:" ë¥¼ "ê²€ìƒ‰:" ìœ¼ë¡œ ë³€ê²½
        cleaned = cleaned.replace('ë§í¬:', 'ê²€ìƒ‰:')
        cleaned = cleaned.replace('- ë§í¬:', '- ê²€ìƒ‰:')
        
        # ğŸ”¥ ë¹ˆ ë§í¬ ë¼ì¸ ì •ë¦¬
        lines = cleaned.split('\n')
        filtered_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not (line.startswith('ê²€ìƒ‰:') and len(line) < 20):
                filtered_lines.append(line)
        
        cleaned = '\n'.join(filtered_lines)
        
        # ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if len(cleaned.strip()) < 50:
            return get_default_content('references')
        
        return cleaned.strip()
    except:
        return get_default_content('references')

def parse_text_response(text):
    """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ë³„ë¡œ ì¶”ì¶œ"""
    return manual_parse_sections(text)

def create_error_response():
    """ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
    return {
        "abstract": "ë…¼ë¬¸ ì´ˆë¡ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì£¼ì œë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        "introduction": "ì„œë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ë°°ê²½ì„ ë‹¤ì‹œ ê²€í† í•´ì£¼ì„¸ìš”.",
        "methods": "ì—°êµ¬ ë°©ë²• ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        "results": "ì˜ˆìƒ ê²°ê³¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
        "visuals": "ì‹œê°ìë£Œ ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "conclusion": "ê²°ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "references": "ì°¸ê³ ë¬¸í—Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    }
