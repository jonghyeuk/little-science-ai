import urllib.parse
import feedparser
import streamlit as st
from utils.explain_topic import explain_topic  # âœ… GPT ì„¤ëª… ì¶”ë¡ 
# âœ… arXiv ë…¼ë¬¸ ê²€ìƒ‰ ë° ìš”ì•½ ì¶”ë¡ 
def search_arxiv(query, max_results=5):
    base_url = "http://export.arxiv.org/api/query"
    encoded_query = urllib.parse.quote(query)
    query_url = f"{base_url}?search_query=all:{encoded_query}&start=0&max_results={max_results}"
    try:
        # ğŸ” Feed íŒŒì‹±
        feed = feedparser.parse(query_url)
        entries = feed.entries
        if not entries:
            return [{
                "title": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                "summary": "í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ arXiv ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "link": "",
                "source": "arXiv"
            }]
        # ğŸ“Œ ê²°ê³¼ êµ¬ì„±
        results = []
        for entry in entries:
            title = entry.title
            summary = entry.get("summary", "")
            link = entry.link
            # ğŸ¤– GPTë¡œ ìš”ì•½ ì¶”ë¡ 
            try:
                explanation_lines = explain_topic(title)
                explanation = explanation_lines[0] if explanation_lines else summary
            except Exception:
                explanation = summary or "ìš”ì•½ ì •ë³´ ì—†ìŒ"
            results.append({
                "title": f"{title}",
                "summary": explanation,
                "link": link,
                "source": "arXiv"
            })
        return results
    except Exception as e:
        st.error(f"âŒ arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [{
            "title": "arXiv ê²€ìƒ‰ ì‹¤íŒ¨",
            "summary": "",
            "link": "",
            "source": "arXiv"
        }]
